---
title: "What makes wine great?"
author: "Yuga Hikida, Adya Maheshwari"
date: "2024-01-02"
output: 
  beamer_presentation:
  theme: "Boadilla"
  innertheme: "circles"
header-includes:
  - \definecolor{UBCblue}{rgb}{0.04706, 0.13725, 0.26667} % UBC Blue (primary)
  - \usecolortheme[named=UBCblue]{structure}
  - \setbeamercolor{frametitle}{fg=white,bg=UBCblue}
  - \setbeamercolor{titlelike}{parent=structure,fg=UBCblue}
  - \setbeamertemplate{footline}[page number]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r include = FALSE}
library(ggplot2)
library(tidyr)
library(dplyr)
library(bayesplot)
library(brms)
library(patchwork)
library(latex2exp)
```

```{r include = FALSE}
fit1 <- readRDS("results/fit1.rds")
fit2 <- readRDS("results/fit2.rds")
fit3 <- readRDS("results/fit3.rds")
```


```{r include = FALSE}
d <- read.csv("data/winequality-white.csv", sep = ";")
```

## Task

- Prediction of quality of (white) wine (from 1, 2,.. up to 10) using physicochemical variables.

```{r}
ggplot(d, aes(x = factor(quality))) +
  geom_bar(stat = "count", alpha = 0.7) +
  labs(title = "", x = "", y = "count") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 15))
```


## Data: Preditictive variables

```{r}
df_long <- d %>%
  select(!quality) %>%
  tidyr::gather(key = "variable", value = "value")

ggplot(df_long, aes(x = value, fill = variable)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~variable, scale = "free") +
  theme_minimal() +
  theme(legend.position = "none", text = element_text(size = 12)) +
  labs(x = "", y = "", title = "", subtitle = "")
```



## How to model "quality"?

- $M_1$: Categorical variable. $quality \in \{`1`, ..., `10` \}$

  $\Rightarrow$ Classification
  
- $M_2$: Continuous variable. $quality \in \lbrack1, 10 \rbrack$
 
  $\Rightarrow$ Regression
  
- $M_3$: Ordered Categorical variable. $quality \in \{1, ..., 10 \}$

  $\Rightarrow$ Ordinal Regression

For following slides, $y$ for $quality$ and $X$ for (vector of) predictive variables.

## $M_1$: Classification (1)
\[
\begin{aligned}
 y &\sim \text{categorical}(\psi_1,...,\psi_C) 
   &= \prod_{c=1}^{C} \psi_c^{I_{c(y)}}
\end{aligned}
\]

where $C$ is the number of categories ($C = 7$ for our case), $\psi_c = Pr(y = c)$ such that $\sum_{c=1}^{C} \psi_c^{I_c(y)} = 1$, and

$$
 I_{c(y)} =
    \begin{cases} 
    1 & y = c \\
    0 & \text{otherwise}
    \end{cases}
$$

## $M_1$: Classification (2)
For $c = 1,..,C$:
\[
\begin{aligned}
 \psi_c &= \text{softmax}(\eta_c) \\
  &= \frac{e^{\eta_c}}{\sum_{k=1}^{C} e^{\eta_k}} \\
 \eta_c &= X_c\beta_c \;\; \text{where}  \;\; X_c = X[y == c] \\
 \beta_c &\sim \text{Normal}(0, \sigma^2I)
\end{aligned}
\]

```{r eval=FALSE, echo = TRUE}
f <- quality ~ citric.acid + residual.sugar +
   total.sulfur.dioxide + free.sulfur.dioxide + 
   chlorides + density + pH + sulphates + alcohol + 
   fixed.acidity + volatile.acideity

fit1 <- brm(f, 
            data = d, 
            family = categorical(link = "logit"),
            prior = p1)
```


## $M_1$: Result (1)

```{r message = FALSE}
var_look_1 <- "residual.sugar"
pl1_1 <- mcmc_areas(fit1, paste("b_mu", 4:9, "_", var_look_1, sep = "")) +
         scale_y_discrete(labels = paste(4:9)) +
         labs(y = paste("beta for", var_look_1))

var_look_2 <- "alcohol"
pl1_2 <- mcmc_areas(fit1, paste("b_mu", 4:9, "_", var_look_2, sep = "")) +
         scale_y_discrete(labels = paste(4:9)) +
         labs(y = paste("beta for", var_look_2))

pl1_1 + pl1_2
```

## $M_1$: Result (2)

```{r include = FALSE}
pl1_5 <- plot(conditional_effects(fit1, effects = var_look_1, method = "posterior_epred", categorical = TRUE))[[1]]
pl1_6 <- plot(conditional_effects(fit1, effects = var_look_2, method = "posterior_epred", categorical = TRUE))[[1]]
```
```{r}
(pl1_5 + theme(legend.position = "none")) + pl1_6
```



## $M_2$: Regression
We choose to use Normal distribution but other distribution such as t-distribution can be also chosen.

\[
\begin{aligned}
 y &\sim \text{Normal}(\eta, \gamma^2) \\
 \eta &= x^T\beta \\
 \beta &\sim \text{Normal}(0, \sigma_\beta^2I) \\
 \gamma^2 &\sim \text{Half-normal}(0, \sigma_\gamma^2)
\end{aligned}
\]

```{r eval=FALSE, echo = TRUE}
fit2 <- brm(f, 
            data = d,
            family = gaussian(),
            prior = p2)
```

## $M_2$: Result (1) ##
```{r message = FALSE}
pl2_3 <- mcmc_areas(fit2, "b_residual.sugar") + scale_y_discrete(labels = "") + labs(title = "residual.sugar")
pl2_4 <- mcmc_areas(fit2, "b_alcohol") + scale_y_discrete(labels = "") + labs(title = "alcohol")
pl2_3 / pl2_4
```

## $M_2$: Result (2) ##
```{r include = FALSE}
pl2_1 <- plot(conditional_effects(fit2, "residual.sugar", method = "posterior_epred"))[[1]]
pl2_2 <- plot(conditional_effects(fit2, "residual.sugar", method = "posterior_predict"))[[1]]
```
```{r}
(pl2_1 + labs(title = "Expected value")) / pl2_2 + labs(title = "PP draws")
```



## $M_3$: Ordinal Regression: Cumulative Model (1)
For $c = 1,..,C$: 
\[
\begin{aligned}
 \psi_c &= Pr(y \leq c) - Pr(y \leq c - 1) \\
 &:= Pr(\tilde{y} \leq \tau_c) - Pr(\tilde{y} \leq \tau_{c - 1}) \\
 \tilde{y} &= \eta + \epsilon, \; \epsilon \sim \text{Normal}(0, 1) \\
 \beta &\sim \text{Normal}(0, \sigma^2I) \\
 \tau_c &\sim \text{Normal}(0, \sigma_{\tau_c}^2)
\end{aligned}
\]

## $M_3$: Ordinal Regression: Cumulative Model (2)
Other expression:
\[
\begin{aligned}
 Pr(\tilde{y} \leq \tau_c)
 &= Pr(\eta + \epsilon \leq \tau_c) \\
 &= Pr(\epsilon \leq \tau_c - \eta)  \\
 &= \Phi(\tau_c - \eta) \;\; \Phi: \text{cdf of standard normal aka probit}
\end{aligned}
\]

Then we have:

\[
\begin{aligned}
 \psi_c &= \Phi(\tau_c - \eta) - \Phi(\tau_{c - 1} - \eta) \\
 &\vdots\\
\end{aligned}
\]

```{r eval=FALSE, echo = TRUE}
fit3 <- brm(f, 
            data = d,
            family = cumulative("probit"),
            prior = p3)
```


## $M_3$: Result

```{r include = FALSE}
pl3_1 <- plot(conditional_effects(fit3, effects = "alcohol", categorical = TRUE, plot = FALSE))[[1]]
pl3_2 <- plot(conditional_effects(fit3, effects = "alcohol", method = "posterior_linpred", plot = FALSE))[[1]]
```

```{r}
pl3_1 + (pl3_2 + labs(y = TeX(r"(\tilde{y})")))
```

## Model Comparison

leave-one-out CV
```{r warning=FALSE, message=FALSE, echo = TRUE, comment = ""}
loo_compare(fit1, fit2, fit3)
```

Posterior Model Probability
```{r warning=FALSE, message=FALSE, echo = TRUE, comment = "", results = FALSE}
pmp <- post_prob(fit1, fit2, fit3)
```

```{r, comment = ""}
pmp
```



## Partial Pooling
- Adding data of red wine and do partial pooling.
- 

## Adding non-linearity





